---
title: 论文阅读记录（二）
toc: true
clearReading: true
thumbnailImagePosition: right
metaAlignment: center
categories: [文章, 联邦学习]
tags: 联邦学习
keywords:
  - 联邦学习
  - Non-iid
excerpt: 2021-11月15日至2021-11月18日的论文阅读笔记
date: 2021-11-22 16:54:29
thumbnailImage:
---

<!-- toc -->

<!--toc-->

## 进展

本周主要确定下来几件事情：

1. 第一周看的论文太快，太粗，并且没有及时做总结，好多都只是过了一遍，本周开始好好补笔记:joy:
2. 原来`personalization layer`或者`CFL`更多的是一种`multitask Learning`，查看了`multitask learning`的相关论文
3. 本周有点混，找实习的事情

## 论文笔记

### Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints

本文是从 Federated learning with hierarchical clustering of local updates to improve training on non-IID data 引用中查询而来，原文中的层次聚类算法看的有点迷糊，学学最初提出聚类联邦学习的文章中的一部分想法

#### research questions

如果参与方之间的数据是异构的，最终 FL 的性能将受到极大的损耗

#### hypotheses

- local clients’ data distributions diverge

#### key findings

- 充分利用 FL 损失函数的分布特性，提出一种基于聚类的联邦学习框架，将不同的参与方分配到不同的聚类中，各不同的聚类协同训练一个 FL 模型；不同于**联邦多任务学习**，聚类联邦学习并不需要更改联邦学习中的通信协议，非常适合于`nonconvex objectives`，并不需要提前知道聚类的先验知识，并且有强大的数学知识对聚类的保证，所以此论文就提出了聚类联邦学习
- CFL 非常灵活，可以处理随时间变化的客户群体，并且可以以隐私保护的方式实现
- 于聚类只在 FL 收敛到一个平稳点之后进行，所以 CFL 可以被看作是一种后处理方法，通过允许客户得到更专业化的模型，它总能达到比传统 FL 更高或同等的性能；并且讨论了什么时候进行聚类可以取得比较好的结果
- 本文还提出了基于差分隐私的 CFL 方法，目前还没有查看

如果每个网络的初始化部分不相同，比如一个初始化最大值，另一个初始化为最小值，最优点在中间，那么这两个网络的优化方向就是完全相反的，但是可能本质上，这两个网络处理的任务相同呀。所以这是不是由于梯度的误判尼？

- 论文中提到，聚类并不是每次汇聚都进行聚类的，而是等到 server 汇聚时，所有的点差不多达到了一个不动点的时候，此时开始进行聚类，此时每个客户端几乎都拥有了相匹配的底层模型，所以不会出现为了同一个目标向中间汇聚的情况发生

### Multitask Learning

[论文下载地址](https://link.springer.com/article/10.1023/A:1007379606734)

本文是从`CFL`的引用中找到的，年代比较久远，类似于一篇`survey`。

假设从同一个领域抽取的任务可能相互包含一定的信息，最终 MTL 的架构如下：

![](https://cdn.jsdelivr.net/gh/pineapple-man/blogImage@main/image/MTL.png)

#### key findings

- 多任务学习能够取得比较好的性能是由于它能够从额外的训练任务中获取其包含的额外信息
- 不相关的任务可以通过充当噪声源提高模型的泛化能力，

### Federated Multi-Task Learning

In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues.

前面一部分还可以看得懂，但是后面设计到底的优化理论太多，有点不知所云，不过给我的启发是：**可以使用相似矩阵来表达众多任务之间的相似性**

### Survey of Personalization Techniques for Federated Learning

- 由于数据的异构型，使得各个客户端的本地模型对本地的测试集的效果当然最好

- 在模型训练的过程增加差分隐私的机制能够对训练过程的隐私进行保护，但是对最终模型的性能有一定的损害

:sailboat:如今对`personalization layer`思想使用的技术主要有两步

1. 通过协作的方式建立一个全局模型
2. 全局模型使用客户端的私有数据为每个客户端的模型进行个性化

:notes:如果想要使联邦学习的`personalization`机制有效，在实践中需要同时解决下面这三个问题

- 开发改进的个性化模型，使大多数客户受益
- 开发一个准确的全球模型，使那些个性化的私人数据有限的客户受益
- 在少量的培训回合中获得快速模型收敛

### Exploiting Shared Representations for Personalized Federated Learning

整体阅读起来还是很轻松的，idea 也非常朴素，就是后面的证明太多（没看）

#### research questions

across clients, the success of centralized deep learning suggests that data often shares a global **feature representation**, while the statistical heterogeneity across clients or tasks is concentrated in the labels

we view the data heterogeneous federated learning problem as n parallel learning tasks that they possibly have some common structure, and our goal is to **learn and exploit this common representation to improve the quality of each client’s model**

:notes:文章的算法 1（FedRep）描述存在一些书写上的问题，不过拥有前后文还是能够轻易判断出

#### key findings

- 利用跨客户机的分布式计算能力，对表示的每次更新执行许多低维局部参数的局部更新，学习数据分布之间的共享低维表示
- 对 base layer 与 personalized layer 分别迭代更新，个人感觉不太好

[「ICML2021」| 联邦学习系列 — Exploiting Shared Representations for Personalized Federated Learning](https://zhuanlan.zhihu.com/p/384706200)

## 额外补充的知识

|                                                          Non-iid                                                          |                                                   其他                                                   |                                                  优化                                                   |
| :-----------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: |
|             [联邦学习中常见的 Clients 数据 Non-IID 非独立同分布总结](https://zhuanlan.zhihu.com/p/419746591)              | [How a Kalman filter works, in pictures](https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/) | [凸优化\|笔记整理（1）——引入，优化实例分析，凸集举例与相关性质](https://zhuanlan.zhihu.com/p/194308254) |
|                   [Dataset Shift In Machine Learning—目录与前言](https://zhuanlan.zhihu.com/p/68645786)                   |             [联邦学习 Federated Learning 思考（一）](https://zhuanlan.zhihu.com/p/388621102)             |      [数值优化\|笔记整理（1）——引入，线搜索：步长选取条件](https://zhuanlan.zhihu.com/p/118443321)      |
| [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/#PriorFigure) |           [「ICML 2021」联邦学习相关论文导读(待更新)](https://zhuanlan.zhihu.com/p/378557472)            |                                                                                                         |
|                                                                                                                           |               [「联邦学习」— Personalization 技术](https://zhuanlan.zhihu.com/p/354527722)               |                                                                                                         |
|                                                                                                                           |              [0 范数、1 范数、2 范数有什么区别？](https://www.zhihu.com/question/20473040)               |                                                                                                         |

## 遗留的问题

:grey_question:本周还是对问题的本质定义存在一些问题

1. 到底什么是数据的非独立同分布`Non-IID`？
2. 目前我应该研究什么样的 $Non-iid$？或者说，$Non-iid$有哪些分类？

_Advances and Open Problems in Federated Learning_，论文中对 $Non-iid$ 进行了一些阐述

For the following, consider a supervised task with features $x$ and labels $y$. A statistical model of federated learning involves two levels of sampling: accessing a datapoint requires first sampling a client $i \sim Q$, the distribution over available clients, and then drawing an example $(x, y) \sim P _{i}(x, y)$ from that client's local data distribution.

那么 $Non-iid$ 指的就是

$$
P _{i}(x, y)\neq P _{j}(x, y)
$$

根据条件概率公式

$$
P(x, y) = P(x \mid y)P(y)
$$

所以上面的不等式就变成了:

$$
P_i(x \mid y)P_i(y) \neq P_j(x \mid y)P_j(y)
$$

那么根据以上存在的两种变量可能存在四种不等的方式

### Feature distribution skew(covariate shift)：

$P_i(y) \neq P_j(y)$ 但是$P(y \mid x) $ 可以所有客户端共享的，比如，在手写体识别领域，一个人写相同的数字，可能也会由于笔画宽度、斜度等不同因素造成写出来的数字不太相似；

### Label distribution skew (prior probability shift):

The marginal distributions $P_i(y)$ may vary across clients, even if $P(x \mid y)$ is the same

各个客户端的数据样本不一致，造成这种偏斜，如果所有客户端的 $p(x \mid y)$ 相同假设成立，那么是否就表示存在一种通用的 feature represetation $P(x)$，使得在已知 $y$ 条件下，这种 $p(x \mid y)$ 相同，这种 Lable distribution 是常见的问题，也是大家普遍关注的，我也来解决这个问题

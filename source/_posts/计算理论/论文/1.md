---
title: 论文阅读记录（一）
toc: true
clearReading: true
thumbnailImagePosition: right
metaAlignment: center
categories: 为了毕业看的论文
tags: 联邦学习
keywords:
  - 联邦学习
  - Non-iid
excerpt: 2021-11月5日至2021-11月12日的论文阅读笔记
date: 2021-11-15 17:16:32
thumbnailImage:
---
<!--toc-->
> 人一定要靠自己，研究一定要自己做，多讨论多请教

## 进展

本周主要确定下来几件事情：

1. 之后的研究方向为联邦计算场景下的`Non-IID`问题，更好的提高最终训练模型效用
2. 快速阅读文献，了解领域知识

## 论文笔记

### Federated Learning with Non-IID Data

本文是联邦学习处理 `non-iid` 论文中引用量最多的一篇文章，本身并没有太多巧妙的算法，不过是给出了一种看似可行的方案，但是这种基于`data sharing`的算法，最终必然不能很好的满足联邦学习的隐私性问题

#### research questions

> In this work, we focus on the statistical challenge of federated learning when local data is non-IID

#### hypotheses

- 各个参与方希望学习一个**全局共享的预测模型，同时数据不出本地**
- 聚合方存在一个全局数据集，训练数据的分布与这个全局数据集相同

#### method/measures used

- 训练集被均匀地划分为10个参与者，如果是`IID`场景，则这十个参与者从训练集中均匀的随机采样等量的数据集；而在本文中提出了两种 `Non-IID` 方式：

1. 十个参与者，每个参与者只有一个类的数据（完全 `non-iid`），称这种情况为`1-class-non-IID`
2. 将数据整理后平均分隔成 20 个小数据区（每一个这样的小数据区是只有一类的一半数据），最终每个参与方从这 20 个数据区中随机不放回抽象两个不同类别的小数据区，称为`2-class non-IID`

- 提出了用 `Weight divergence`来衡量 `non-iid` 对权重的影响程度，并给出了详细的证明，为什么`iid`情况下的权重差异和集中式的权重差异不大

$$
\text { weight divergence }=\left\|\boldsymbol{w}^{F e d A v g}-\boldsymbol{w}^{S G D}\right\| /\left\|\boldsymbol{w}^{S G D}\right\|
$$

- 提出了一种`data sharing`的方法，将聚合方的共享数据子集下发到每一个参与方

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/data-sharing-strategy.png)

#### key findings

- 通过提出一种共享数据子集的方法，缓解各参与方数据异构性的挑战（类似于两个没有交集的人，增加相同上下文，随后两个人就有共同话题可以交流）
- 论文对数据的 `Non-IID` 导致准确率降低找到了一种解释：权重发散（`weight divergence`），并且论文使用了`EMD`距离来衡量这种权重发散程度。实验发现，数据异构的网络结构权重与数据同构的网络结构权重，随着层数增加，发散程度也就不断增加
- 下图形象的解释了，为什么`iid`与`non-iid`之间的性能下降会越来越严重

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/weightDivergence.png)

:sparkles:此论文给我的启发点

- 按照师兄的解释，随着网络深度增加，权重逐渐发散，也有可能是由于在浅层网络，执行的是一些简单的提取特征工作，对于相同的数据集，前层网络提取的特征是相同的，所以前面的权重发散程度比较轻微，但是随着网络深度逐渐增加，特征提取的工作逐渐具有针对性，所以出现权重发散也是显而易见的事情

[Federated Learning with Non-IID Data 论文笔记](https://blog.csdn.net/GJ_007/article/details/104768415)

### Federated learning with hierarchical clustering of local updates to improve training on non-IID data

通过层次聚类的方法，将权重相似的参与方聚合起来，最终对聚出来各个类进行平均，而不是对所有参与方的整个聚类进行平均。

:smile:感觉和我最初的想法有点不谋而合

#### research questions

> However in settings where data is distributed in a noniid (not independent and identically distributed) fashion - as is typical in real world situations - the joint model produced by FL suffers in terms of test set accuracy and/or communication costs compared to training on iid data. 

仍然是在数据的`non-iid`场景下，模型的性能会下降，并且在某种`non-iid`场景下，学习一个单独的模型，最终效果并不是最好的

#### hypotheses

联邦学习中各参与方的数据是 `non-iid` 的，如果数据本身就是`iid`分布的，那么最终`FL+HC`算法就会退化到`FedAvg`算法

#### method/measures used

- 实验初始化过程与之前的`non-iid`初始化类似，和`2-class-non-iid`的初始化相同

- 通过引入**层次聚类**的方法，对传统 FL 学习框架提出了一种改进方法。将各参与方分割成不同的聚类，随后通过比对各参与方对本地模型的更新与全局模型权重更新的相似程度进行聚类；一旦将被划分到不同聚类中的参与方，将会在特有的模型上训练，各个不同的聚类将会有不同的模型，并且可以做到这些模型间的并行训练更新
- 每次可以只挑选一部分用户进行训练，并不是全部用户（有点类似于`FedAvg`的一个变体算法），为了能够让所有用户获取最新的全局结果，每过 n 轮就对所有参与者进行一次共识

由于引入了聚类的算法，所以众多的客户端不再是单独的，而是存在一定的内在联系，假设存在 C 各聚类，每个聚类中的任务是相同的，论文中对以上的描述抽象为以下的方式

$$
\begin{equation}
\forall c \in C, \mathbb{E}\_{D_{k}}\left[F_{k}(w)\right]=f\_{c}(w) \text { where } k \in c
\end{equation}
$$



![](https://gitee.com/mingchaohu/blog-image/raw/master/image/FL_HCServer.png)

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/serverAggregation.png)

#### key findings

- 一种能够模型性能的训练方法，可以加速模型收敛，减少通信量
- 每一次聚类的过程都将花费众多的时间，所以论文中提出，**在整个训练过程，仅仅进行一次聚类操作**，随后基于聚类的结果进行分聚类的`FedAvg`算法

- 论文最后提出了后续工作：可以通过增加 DP 来做到隐私保护，同时由于噪声的引入，需要一种更好的聚类的算法，能够抵抗噪声；希望引入压缩方法减少通信开销；最终作者指出或许可以使用此方法来识别恶意客户端

### Federated Learning with Personalization Layers ( AISTATS ，ccf c )

这是阅读的第一篇关于 `personalization layer` 的文章，文章针对 `non-iid` 提出的一种方法，整体简单易懂；

#### research questions

文中提出，统计的异构型将会使得传统的联邦学习算法性能下降，为了解决这种问题，需要设计出一种能够应对异构数据（数据的 `Non-IID` ）仍然保持强健的机器学习算法

#### hypotheses

联邦学习中各参与方的数据是 `non-iid` 的

#### method/measures used

提出了一种新型的网络结构： `base + personalization layer`，使用这种新型网络结构能够有效的抵抗参与方数据的异构性

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/image-20211113171251576.png)

下面是文章中提出的训练算法：

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/image-20211115094741498.png)

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/image-20211115094838658.png)

最终使用 `CIFAR` 数据集进行实验的验证，baseline 选择的是 `FedAvg` 算法；

网络结构：base 层使用的网络是`ResNet-34`或者`MobilieNet-v1`，personalization 层使用的网络是 `FC`以及一些其他的基本网络块，论文中感觉并没有阐述清楚

#### key findings

- 提出了与 `FedAvg` 不同的训练方式
- 发现传统 `iid` 数据集的划分，不适合 `Non-IID` 的情况

:question:网络中参与聚合的 base 层到底学习到了什么？如果每一个参与者的 personalization 层足够的复杂，完全能够应付参与者的任务，那么为什么要有 base Net，并且 base Net 表现出了什么作用？

论文对以上问题进行了消融实验，将复杂的 base Net 变成了简单的 FC 层。如果 base Net 真的是一层冗余架构，所有的事情都是由 personalization 层进行完成的化，更换了底层网络性能应该是不会有明显变化的，但是最终结果却显示，如果没有复杂的底层网络，单独的 personalization 层，是不能学习到具有较高精度的单个客户机模型的，这样也就证明了底层网络的重要性，并不是一种冗余

### Personalized Cross-Silo Federated Learning on Non-IID Data

文章通过交替优化的方式，对个性化层进行更好的选择，优化理论过于沉重，没有太看懂

:sparkles:我认为可以考虑的几点

- 在`Non-IID`情况下进行`peronalized Federated Learning`的瓶颈在于：错误的认为一个全局模型可以适合所有客户端

> We argue that the fundamental bottleneck in personalized cross-silo federated learning with non-IID data is the misassumption of one global model can fit all clients  

- Server端不再进行聚合模型引领协作的进行，反而是保存每个参与方的`psersonalized layer`

> FedAMP allows each client to own a local personalized model, but does not use a single global model on the cloud server to conduct collaborations. Instead, it maintains a personalized cloud model on the cloud server for each client, and realizes the *attentive message passing mechanism* by attentively passing the personalized model of each client as a message to the personalized cloud models with similar model parameters

![](https://gitee.com/mingchaohu/blog-image/raw/master/image/image-20211112103847837.png)

那么每一轮需要迭代提升的就是云端的`personalized layer`如何与更加相似的client进行共享聚合

[论文解读](https://zhuanlan.zhihu.com/p/260776616)

## 额外补充的知识

|                            凸优化                            |                           非凸优化                           |                         机器学习算法                         |                             其他                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| [01 什么是凸优化问题？主要目标和内容是什么？](https://zhuanlan.zhihu.com/p/198020820) | [非凸优化基石：Lipschitz Condition](https://zhuanlan.zhihu.com/p/27554191) | [EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html) | [狄利克雷分布(Dirichlet Distribution)](https://zhuanlan.zhihu.com/p/78743630) |
| [02 凸集及相关基本概念：仿射、凸、凸锥等](https://zhuanlan.zhihu.com/p/198025384) | [为什么我们更宠爱“随机”梯度下降？（SGD）](https://zhuanlan.zhihu.com/p/28060786) |     [EM算法详解](https://zhuanlan.zhihu.com/p/40991784)      | [Dirichlet Distribution（狄利克雷分布）与Dirichlet Process（狄利克雷过程）](http://www.datalearner.com/blog/1051459673766843) |
| [仿射变换与仿射函数](https://zh.wikipedia.org/zh-cn/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2) | [如何在学术能力的自我训练中使用“刻意练习”方法？](https://zhuanlan.zhihu.com/p/422346424) | [错失恐惧症](https://zh.wikipedia.org/wiki/%E9%94%99%E5%A4%B1%E6%81%90%E6%83%A7%E7%97%87) | [文献阅读第一利器：文献笔记法（Literature Notes）](https://zhuanlan.zhihu.com/p/29931530) |

**仿射变换**（Affine transformation），又称**仿射映射**，是指在几何中，对一个向量空间进行一次线性变换并接上一个平移的操作

## 如何做文献笔记

看一篇论文之前需要了解到当前的文章是一篇**heoretical paper （理论性文章）**还是**empirical paper (实证性文章)**，两者的笔记是不同的，**但是我看的大部分文章应该都是empirical paper吧**所以这里主要记录如何对这类文章做笔记

文章的目的不是给出一个全景，不是给出所有问题的答案，不是构建理论框架，而是**只关注某一个假设问题去验证**，只关注某一个或某几个dependent variable (因变量) 和 independent variable （自变量）。这一类文章，我们记笔记的时候要记录：

- `research questions`：文章的研究问题是什么？

- `hypotheses`：文章的实证假设是什么？

- `method/measures used`：该实证用了什么方法收集数据、测量变量、分析数据？

- `key findings`：该研究发现了什么研究结果？哪些假设被支持，哪些并没有成立？

除了以上是文章提供的信息外，还应该记录以下

- 有哪些重要的观点你想要记住，或是将来可能会引用到 （key citations）
- 有哪些结论你将来可能用到；
- 有哪些方法你将来可能用到；
- 文章在研究设计上有哪些不足?有没有更好的改进方法？
- 文章让你想到了哪些观点类似或者完全不同的其他文章？
- 你对文章中观点、论述、方法、讨论等部分有什么想法和批判？

## 本周遗留问题

:grey_question:以下记录本周学习过程中的疑惑，以及自己给自己的回答

1. 多个client进行训练，最终的`test acc`如何评判？

- 每个client的训练任务不同，所以最终单独计算各个client的`test acc`然后求平均
- 虚拟出一个新的`client`，在新的`client`上单独进行`test`检验

2. 到底应该先考虑全局模型（`global model`）的性能还是局部模型（`local model`）的性能？

- （个性化）联邦学习，最终的目的是增强本地模型的性能，并不是增强全局模型的性能。通过横向联邦后，更多的是希望抹除各个参与方之间的差异性，获得各个参与方之间的共性。能够使得本地模型精度更高。通过个性化联邦学习之后，并不是让各参与方的模型更具有泛化能力，比如：只能识别数字`1`的参与方，通过联邦学习之后不应该能够识别出数字`2`或者其他的东西，最终做到的效果是通过联邦学习之后能够让只能够识别`1`的参与方的精度提高，就足够了
- 但如果是单纯的横向联邦学习，各个参与方的共同目的就是训练一个比较强大的全局模型，这个时候就是需要以提高全局模型性能为目的进行网络设计

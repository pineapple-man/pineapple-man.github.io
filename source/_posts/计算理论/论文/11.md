---
title: 论文阅读记录（十 一）
toc: true
clearReading: true
thumbnailImagePosition: right
metaAlignment: center
categories: 计算机算法理论
tags: 联邦学习
keywords:
  - 联邦学习
excerpt: 本文主要记载2022年02月27日至2022年03月06日的论文阅读笔记
date: 2022-03-06 09:00:00
thumbnailImage:
---
<!-- toc -->

## 进展

 最近主要确定下来对几件事情：



## 论文笔记

### :one: A Secure Federated Transfer Learning Framework

一直在读，就是没有精读下去，本周一定要将本文精读下去，并且复现出来。

#### research questions

如何在联邦场景下设计一个通用的迁移学习框架

#### hypotheses

- 源域的数据存在对应的标签，目标域只有数据不存在标签

- 在源域和目标域存在一些共有的数据样本，也就是说，目标域的中的部分数据共用源域的标签

#### method/measures used

为了能够完善目标域上的数据，定义一个 prediction function 用来给目标域生成标签(:thinking: 既然如此，何不继续使用网络来给目标域生成标签尼)

prediction function 的参数定义以及具体的函数原型如下：
$$
\varphi\left(u\_{j}\^{B}\right)=\varphi\left(u\_{1}^{A}, y\_{1}^{A} \ldots u\_{N\_{A}}^{A}, y\_{N\_{A}}^{A}, u\_{j}^{B}\right)
$$
论文假设，$\varphi\left(u\_{j}\^{B}\right)$ is linearly separable. 因此，整个 prediction function 可以分为如下两个部分：

$$
\varphi\left(u\_{j} ^{B} \right)=\Phi^{A} \mathcal{G}\left(u_{j}^{B}\right)
$$

在这篇文章的实验中，具体使用的函数如下：
$$
\varphi\left(u\_{i}\^{B}\right)=\frac{1}{N\_{1}} \sum\_{i}^{N_{A}} y\_{i}^{A} u\_{i}^{A} \left(u\_{j}\^{B}\right)^{\prime}
$$

对 Loss 进行改造，整体训练的

$$
\underset{\Theta^{A}, \Theta^{B}}{\operatorname{argmin}} \mathcal{L}\_{1}=\sum_{i=1}^{N\_{c}} \ell\_{1}\left(y\_{i}^{A}, \varphi\left(u\_{i}^{B}\right)\right)
$$

where $\Theta^{A}, \Theta^{B}$ are training parameters of $N e t^{A}$ and $N e t^{B}$, respectively. Let $L_{A}$ and $L_{B}$ be the number of layers for $N e t^{A}$ and $N e t^{B}$, respectively. 
Then, 
$$
\Theta\^{A}=\left\\{\theta\_{l}\^{A}\right\\}\_{l=1}^{L_{A}},
\Theta\^{B}=\left\\{\theta\_{l}\^{B}\right\\}\_{l=1}^{L_{B}}
$$
where $\theta\_{l}^{A}$ and $\theta\_{l}^{B}$ are the training parameters for the $l$ th layer. $\ell_{1}$ denotes the loss function. For logistic loss, $\ell\_{1}(y, \varphi)=\log (1+$ $\exp (-y \varphi)$ )。$\ell\_{1}$类似目标域中的 prediction loss

In addition, we also aim to minimize the alignment loss between $\mathrm{A}$ and $\mathrm{B}$ in order to achieve feature transfer learning in a federated learning setting:
$$
\underset{\Theta^{A}, \Theta^{B}}{\operatorname{argmin}} \mathcal{L}\_{2}=\sum\_{i=1}^{N\_{A B}} \ell\_{2}\left(u\_{i}^{A}, u\_{i}^{B}\right)
$$
where $\ell_{2}$ denotes the alignment loss. Typical alignment losses can be 
$$
-u\_{i}\^{A}\left(u\_{i}\^{B}\right)\^{\prime} or   
\left\\|u\_{i}\^{A}-u\_{i}\^{B}\right\\|\_{F}^{2}
$$
For simplicity, we assume that it can be expressed in the form of 
$$
\ell\_{2}\left(u\_{i}\^{A}, u\_{i}\^{B}\right)=\ell\_{2}\^{A}\left(u\_{i}^{A}\right)+\ell\_{2}\^{B}\left(u\_{i}\^{B}\right)+\kappa u\_{i}\^{A}\left(u\_{i}\^{B}\right)\^{\prime}
$$

总体的 loss 组成如下：
$$
\underset{\Theta^{A}, \Theta^{B}}{\operatorname{argmin}} \mathcal{L}=\mathcal{L}\_{1}+\gamma \mathcal{L}\_{2}+\frac{\lambda}{2}\left(\mathcal{L}\_{3}\^{A}+\mathcal{L}\_{3}\^{B}\right)
$$

where $\gamma$ and $\lambda$ are the weight parameters, and $\mathcal{L}\_{3}\^{A}=\sum\_{l}\^{L\_{A}}\left\\|\theta\_{l}\^{A}\right\\|\_{F}\^{2}
$ and $\mathcal{L}\_{3}\^{B}=\sum\_{l}\^{L\_{B}}\left\\|\theta\_{l}\^{B}\right\\|\_{F}\^{2}$ are the regularization terms. Now we focus on obtaining the gradients for updating $\Theta^{A}$, $\Theta^{B}$ in back propagation. For $i \in\\{A, B\\}$, we have:

$$
\frac{\partial \mathcal{L}}{\partial \theta\_{l}\^{i}}=\frac{\partial \mathcal{L}\_{1}}{\partial \theta\_{l}\^{i}}+\gamma \frac{\partial \mathcal{L}\_{2}}{\partial \theta\_{l}\^{i}}+\lambda \theta\_{l}\^{i}
$$

Under the assumption that $\mathrm{A}$ and $\mathrm{B}$ are not allowed to expose their raw data, a privacy-preserving approach needs to be developed to compute equations objective function and back propagation gradients . Here, we adopt a second order Taylor approximation for logistic loss:
$$
\ell_{1}(y, \varphi) \approx \ell_{1}(y, 0)+\frac{1}{2} C(y) \varphi+\frac{1}{8} D(y) \varphi^{2}
$$

and the gradients is :
$$
\frac{\partial \ell_{1}}{\partial \varphi}=\frac{1}{2}C(y)+\frac{1}{4}D(y)\varphi
$$
Where $C(y) = -y,D(y) = y^2$



### :two: Adversarial Multiple-Target Domain Adaptation for Fault Classification

过去数据驱动的故障诊断技术基于这样一种假设：训练数据和测试数据具有相同的分布。实际上，不同机器拥有各种工况，这就导致了域迁移现象，虽然目前域自适应方法已经能够解决这个问题，但是当前的域自适应方法仅适用于单源域单目标域单情况。

单源域单目标域的方法，对于每一个新的目标域都需要重新训练，泛化能力较差。因此，本文想要提出一种单源域多目标域的方法，通过单一的源域训练，可以泛化到多目标域的场景。

## :three: FEDERATED ADVERSARIAL DOMAIN ADAPTATION

理论分析比较深入，并且使用了 attention 机制来自定自动划定权限系数，下周继续研读

## 额外补充的知识

|                           深度学习                           |                           机器学习                           |                           pytorch                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| [生成对抗网络 – Generative Adversarial Networks \| GAN](https://easyai.tech/ai-definition/gan/) | [理解线性可分和线性不可分与机器学习什么叫线性模型](https://blog.csdn.net/qq_45079973/article/details/104051441) | [pytorch 给数据增加一个维度](https://blog.csdn.net/itnerd/article/details/101564698) |
| [概述領域自適應 (Domain Adaptation)_李弘毅_ML2021#12](https://medium.com/@ze_lig/%E6%A6%82%E8%BF%B0%E9%A0%98%E5%9F%9F%E8%87%AA%E9%81%A9%E6%87%89-domain-adaptation-%E6%9D%8E%E5%BC%98%E6%AF%85-ml2021-12-5c3e2ee932e) | [线性可分性、线性不可分性的定性理解](https://www.cnblogs.com/MyUniverse/p/10147187.html) | [pytorch求范数函数——torch.norm](https://www.cnblogs.com/wanghui-garcia/p/11266298.html) |
| [为什么网络越深越难训练](https://blog.csdn.net/qq_35900810/article/details/106048064) | [多元函数的泰勒展开式](https://zhuanlan.zhihu.com/p/33316479) | [Pytorch 查看模型参数](https://analytics.google.com/analytics/web/#/) |
| [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) | [Frobenius范数的定义？](https://www.zhihu.com/question/22475774) | [pytorch 打印网络回传梯度](https://blog.csdn.net/Jee_King/article/details/103017077) |
|    [详解残差网络](https://zhuanlan.zhihu.com/p/42706477)     |      [矩阵求导](https://fei-wang.github.io/matrix.html)      | [PyTorch中使用指定的GPU](https://www.cnblogs.com/darkknightzh/p/6836568.html) |
| [详解Transformer （Attention Is All You Need）](https://zhuanlan.zhihu.com/p/48508221) | [联邦学习 OR 迁移学习？No，我们需要联邦迁移学习](https://www.jiqizhixin.com/articles/2020-11-18-9) | [Pytorch训练模型得到输出后计算F1-Score 和AUC](https://www.yuque.com/books/share/9f4576fb-9aa9-4965-abf3-b3a36433faa6/td13lh) |
|    [详解残差网络](https://zhuanlan.zhihu.com/p/42706477)     |      [矩阵求导](https://fei-wang.github.io/matrix.html)      | [Pytorch神经网络入门（1）参数的手动更新和自动更新](https://blog.csdn.net/OldDriver1995/article/details/115125476) |
| [通俗理解生成对抗网络GAN](https://zhuanlan.zhihu.com/p/33752313) | [二分类和多分类问题的评价指标总结](https://blog.csdn.net/wf592523813/article/details/95202448) |   [python注释规范](https://www.jianshu.com/p/4facd9ff2fcd)   |
| [ML Lecture 19: Transfer Learning](https://www.youtube.com/watch?v=qD6iD4TFsdQ) | [「详解GAN」为什么训练生成对抗网络如此困难！](https://easyai.tech/blog/gan-why-it-is-so-hard-to-train/) | [Autograd (4)：PyTorch 高阶自动导数](https://ajz34.readthedocs.io/zh_CN/latest/ML_Notes/Autograd_Series/Autograd_Hess.html) |
| [生成对抗网络GANs学习路线](https://mp.weixin.qq.com/s/jtkz_GA621PfUBbVyNDxUA) | [生成性对抗网络（GAN）初学者指南 – 附代码](https://easyai.tech/blog/generative-adversarial-networks-gans-a-beginners-guide/) | [pytorch tensor张量维度转换（tensor维度转换）](https://blog.csdn.net/x_yan033/article/details/104965077) |
